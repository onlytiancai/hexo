---
title: '202307'
date: 2023-07-05 09:01:51
tags:
---
## english

余光中：怎样改进英式中文？- 论中文的常态与变态
https://open.leancloud.cn/improve-chinese/

前谷歌工程师的英语频道
https://www.youtube.com/watch?v=pDehb5xQfWc

英语口语教程
https://www.youtube.com/@SpeakEnglishWithTiffani/videos

https://engineering.linkedin.com/blog/2021/leveraging-behavior-analytic-computation-for-anti-abuse-defenses

Behavior analytic computation, on the other hand, can provide high quality features for bot-driven attacks without collecting extra user data. 

The idea of behavior analytic computation is to capture the key bot/automation signatures hidden among the existing feature values or numbers—these signatures can be preserved even if the features are constantly changed.

It intelligently utilizes simple arithmetic and logarithmic operations to transform existing features into more robust ones in constant time. 
Two key operations of behavior analytic computation are ratio and logarithm. 
Ratio can capture the relative change of numbers instead of absolute numbers; 
logarithm can amplify particular ratio values that differentiate bot/automation traffic and human traffic patterns.

When two features are independent of each other, statistically we will see some kinds of natural distributions between them.
Assuming X and Y are two feature types, xi and yi are values of the feature, respectively. 
When a particular value x1 is selected and the request numbers from top values of Y within a certain time period are sorted descendingly, the distribution of different yi count can be obtained.

Another example where behavior analytic computation can contribute is by improving the quality of counters. 
Counters are commonly used against large-scale abuse, 
as higher counter values typically reflect the likelihood of automation-driven abuse.

However, certain behavioral and structural information that is not captured by counters will also be missed by models if only typical counters are used as the features.

Fig. 2 shows two examples of the requests from a single user within one hour where the x-axis is time and y-axis is the request index.

The counter values for both cases are identical, but different behaviors are shown.

While there is no obvious automation evidence found on Fig a, the requests on Fig b contain certain regular patterns indicating automation behind the scenes.

Behavior analytic computation again captures such a plot signature into a single automation score where more human-like plots (Fig. 2(a)) have lower scores and automation-like plots (Fig. 2(b)) have higher scores. 

The extracted score is able to capture the key automation pattern even if users constantly adjust different regularities within the same time period.

The same lightweight computation continues to provide speed and flexibility to the defense. 

More importantly, the extracted automation score is able to fill the gap where typical counters fail to differentiate between human requests or bot-driven automation requests at low counting numbers.

Due to its capability to detect low-frequency automation, this extracted automation score has almost doubled our machine learning model recall on detecting automation-driven abuse at LinkedIn.

Overall, we are able to leverage behavior analytic computation for our anti-abuse defense from three different aspects:

Behavior analytic computation can capture automation signatures as robust features, 
thus improving model and rule resilience against fast-changing attacks. 
When defending against an actual dynamic attack, 
the extracted feature can bring significant robustness improvements 
- greatly extending the time of protection by more than two orders of magnitude.

The other key merit of behavior analytic computation is the ability to expand abuse labels. 
The typical anti-abuse defense is based on supervised models. 
However, for many bot-driven abuse types, such as scraping or account takeover, 
often only a small portion of all abusive requests/users are labeled, 
which limits the effectiveness of the models.

Behavior analytic computation 
provides an unsupervised approach 
where each extracted automation signature 
can perform as an automation detection classifier. 
Those signatures have helped 
expand the overall recall 
when defending against bot-driven large-scale attacks 
in many abuse verticals at LinkedIn.

Overall, we are able to 
leverage behavior analytic computation 
for our anti-abuse defense 
from three different aspects:

As bad actors can 
persistently adapt to penetrate existing defenses 
and additional applications on the platform 
can provide entry points 
for new abuse scenarios, 
anti-abuse defense 
inevitably needs to continually handle 
new abuse scenarios and behaviors.

Quick turnaround to the new abuse activities 
becomes critical to properly contain the overall damage. 
Unlike other robust features, 
such as browser or device fingerprinting, 
which often require longer engineering time and higher cost to onboard, 
behavior analytic computation 
can provide a prompt response 
to evolving attack signals 
to mitigate the threat.

When facing new abuse scenarios 
where early assessment of existing features 
is critical to define anti-abuse strategy, 
behavior analytic computation’s lightweight extraction 
can also offer fast offline analysis 
and quick preliminary defense onboarding.

Bad actors will always continue to change 
and evolve their behaviors, 
so resilience of the defense and fast turnaround 
are essential to ensure long lasting anti-abuse success. 
Behavior analytic computation is a newer concept 
that offers a lightweight and quick defense onboarding 
based on existing features, 
while preserving member privacy. 

The extracted automation signatures 
can not only enrich the feature set, 
enhancing defense robustness, 
but also provide expanded abuse labels 
to significantly improve incident response time and coverage 
for large-scale bot-driven attacks.

Fighting abuse at LinkedIn 
is an ongoing effort 
requiring extensive collaboration 
between multiple teams. 
I would like to acknowledge our partners from the following teams, 
without whom this would not have been possible: 

## 架构

技术架构评审需要考虑的问题-朱晔

- 组件选型
    - A 不是开源的，出了问题怎么办？
    - B 虽然是开源的，但是是 Erlang 写的，公司没人能看懂怎么办？
    - C 我看待解决的 Issues 还有很多，有没有去了解过？
    - 这个组件在性能方面你是否了解过？
    - 开源的免费版本不支持集群怎么办？
    - 如果彻底要自己写这个组件有没有可能性？
- 性能
    - 整体设计上会做到的 TPS、QPS 和 RT 是多少？
    - 随着数据量的增大系统性能会不会出现明显问题？
    - 系统哪个环节会是最大的瓶颈？
    - 是否打算做压力测试，压力测试方案是怎么样的？
    - 怎么提高前端用户的访问流畅性？
- 可伸缩性
    - 每一个环节是否都是可以横向扩展的？
    - 扩容需要怎么做手动还是自动？
    - 数据库不能横向扩展怎么办？
    - 纵向扩展有多少效果？
    - 横向扩展是否是线性的？
    - 扩展后是否可以提高响应速度？
- 灵活性
    - 是否有了解过产品层面以后会怎么发展？
    - 模块 A 是否能拆分出去独立为其它业务服务？
    - 模块 B 是否可以替换为另一种第三方数据源？
    - 如果流程有变，需要多大的工作量来适应？
    - 业务是否可以做到可配？
- 可扩展性
    - 为什么 A 和 B 都有差不多的逻辑？
    - 是否考虑到了 A 业务的实现以后还有 B 的可能性？
    - 如果现在有两种策略以后扩展到了八种策略怎么做？
    - 以后是否可以把这个业务的 H5 前端适配到 PC？
- 可靠性
    - 是否架构中有单点？
    - 故障转移是怎么实现的？
    - 集群内部故障转移需要多久？
    - MQ 或存储出现问题的时候系统会怎么样？
    - MQ 或存储出现问题又恢复了系统是否会自己恢复？
    - 是否考虑过异地故障转移的方案？
    - 是否考虑过多活的方案？
    - 是否有数据丢失的可能性？
    - 数据丢失后是否可以恢复？
    - 系统彻底挂了对其它业务的影响是什么？
    - 系统彻底挂了是否可以有线下的方式走业务？
- 安全性
    - 是否彻底避免 SQL 注入和 XSS？
    - 是否做了风控策略？
    - 是否有防刷保护机制？
    - 数据库拖库了会怎么样？
    - 是否有数据泄露的可能性？
    - 数据的权限怎么控制的？
    - 功能的权限是怎么控制的？
    - 是否做了日志审计？
    - 受到了 DDOS 攻击怎么办？
    - 数据传输是否加密验签？ 
- 兼容性
    - 老的系统打算怎么办？
    - 怎么进行新老系统替换？
    - 新老系统能否来回切换？
    - 别的系统怎么连接你这套新服务？
    - 上下游依赖是否梳理过，影响范围多大？
    - 上下游改造的难度怎么样？
    - 上下游改造有排期吗？
    - 上下游改造的计划和通知时间确定了吗？
    - 使用了新的数据源数据怎么迁移？
    - 使用了新的技术老项目开发能否适应？
- 弹性处理
    - 这个数据重复消费会怎么样？
    - 这个接口重复调用会怎么样？
    - 是否考虑了服务降级？哪些业务支持降级？
    - 是否考虑了服务熔断？熔断后怎么处理？
    - 是否考虑了服务限流？限流后客户端表现怎么样？
    - 队列爆仓会怎么样？
    - 是否考虑了隔离性？
- 事务性
    - 这段业务由谁保证事务性？
    - 数据库事务回滚后会怎么样？
    - 服务调用了失败怎么办？
    - 队列补偿怎么做的？
    - 服务调用补偿怎么做的？
    - 数据补偿实现最终一致需要多久？
    - 在数据不完整的时候用户会感知到吗？
- 可测试性
    - 测试环境和线上的差异多大？
    - 是否支持部署多套隔离的测试环境？
    - 是否打算做单元测试，覆盖率目标是多少？
    - 测试黑盒白盒工作量的比例是怎么样的？
    - 是否支持接口层面的自动化测试？
    - 是否有可能做UI自动化测试？
    - 压测怎么造数据？
    - 是否可以在线上做压测？
    - 线上压测怎么隔离测试数据？
    - 是否有测试白名单功能？
- 可运维性
    - 每一个组件对服务器哪方面的压力会最大？
    - 重新搭建整套系统最快需要多少时间？
    - 系统是否可以完全基于源代码构建？
    - 系统是否有初始化或预热的环节？
    - 系统里哪些环节需要人工参与？
    - 数据是否需要定期归档处理？
    - 会不会有突发的数据量业务量增大？
    - 随着时间的推移如果压力保持不变的话系统需要怎么来巡检和维护？
    - 怎么在容器里进行部署？
- 监控
    - 业务层面哪些指标需要监控和报警？
    - 应用层面系统内部是否有暴露了一些指标作监控和报警？
    - 系统层面使用的中间件和存储是否有监控报警？
    - 是否所有环节都接入了全链路跟踪？
    - 出现报警的时候应该由谁来处理？
    - 每一个模块是否有固定的主要和次要负责人？
    - 有没有可能系统出了问题无法通过监控指标体现？
    - 哪些指标需要上大屏由监控进行7*24监控？

## mysql

使用delete删除mysql数据的时候，mysql并没有把数据文件删除，而是将数据文件的标识位删除，没有整理文件，因此不会彻底释放空间。被删除的数据将会被保存在一个链接清单中，当有新数据写入的时候，mysql会利用这些已删除的空间再写入。即，删除操作会带来一些数据碎片，正是这些碎片在占用硬盘空间。 -

    optimize table table_name
    alter table table_name engine=engine_name


该方法通常用于切换表的引擎，例如MyISAM转为InnoDB，但是同样适用于释放表空间，只不过切换后的引擎和原来的engine相同罢了。

注意事项

- 这两种方式都适用于我们常用的表，myisam和innodb。
- 优化表的时候会锁表，数据表越大，耗时越长，因此不要在网站忙时进行表优化。

MySQL进阶三板斧（三）看清“触发器 (Trigger)”的真实面目
https://zhuanlan.zhihu.com/p/158670286

How to detect rows with chinese characters in MySQL?
https://stackoverflow.com/questions/9795137/how-to-detect-rows-with-chinese-characters-in-mysql

Notice the Chinese Character is from E4 to E9, hence we use the code

    select number 
    from Chinese_Test
    where HEX(contents) REGEXP '^(..)*(E[4-9])';

## python

如何使用Python画QQ图
https://blog.csdn.net/neweastsun/article/details/125859614

    import numpy as np

    np.random.seed(0)
    data = np.random.normal(0,1, 1000)

    import statsmodels.api as sm
    import matplotlib.pyplot as plt

    fig = sm.qqplot(data, line='45')
    plt.show()

QQ 图的 X 轴是理论分位数，这意味着显示不是实际数据，而是实际数据在正态分布下的位置。
Y 轴显示实际数据。这意味着如果数据值沿45度角的大致直线下降，那么数据分布服从正态分布。

一文让你彻底搞懂最小二乘法（超详细推导）
https://blog.csdn.net/MoreAction_/article/details/106443383

用Python实现最小二乘算法
https://zhuanlan.zhihu.com/p/115648166
https://blog.csdn.net/weixin_43693650/article/details/120295362
https://blog.csdn.net/weixin_44231148/article/details/107149604
https://www.cnblogs.com/NanShan2016/p/5493429.html

    from scipy.optimize import leastsq

    def fun(p,x): #回归模型函数
        k,b = p
        return k*x+b

    def error(p,x,y): #误差
        return fun(p,x)-y

    p0 = np.array([1,3])
    para = leastsq(error,p0,args=(Xi,Yi))

    k,b = para[0]

- 现实生活中我们可以利用最小二乘法解决更为复杂的问题。
- 比方说有一个未知系数的二元二次函数 f(x,y)=w0x^2+w1y^2+w2xy+w3x+w4y+w5，这里 w0~w5 为未知的参数，为了确定下来这些参数，将会给定一些样本点 (xi,yi,f(xi,yi))，然后通过调整这些参数，找到这样一组 w0~w5，使得这些所有的样本点距离函数f(x,y)的距离平方之和最小。
- 至于具体用何种方法来调整这些参数呢？有一种非常普遍的方法叫“梯度下降法”，它可以保证每一步调整参数，都使得f(x,y)朝比当前值更小的方向走，只要步长 α 选取合适，我们就可以达成这种目的。
- 而这里不得不提的就是神经网络了。神经网络其实就是不断调整权值 w 和偏置 b，来使得cost函数最小，从这个意义上来讲它还是属于最小二乘法。
- 更为可爱的一点是，神经网络的调参用到的仍是梯度下降法，其中最常用的当属随机梯度下降法。
- 而后面伟大的 bp 算法，其实就是为了给梯度下降法做个铺垫而已，bp 算法的结果是 cost 函数对全部权值和全部偏置的偏导，而得知了这些偏导，对于各个权值 w 和偏置 b 该走向何方就指明了方向。
- 因此，最小二乘法在某种程度上无异于机器学习中基础中的基础，且具有相当重要的地位。

线性回归是确定两种及两种以上变量的相互依赖关系。在数据分析中，线性回归是最简单且最有效的分析方法。
举个简单的例子，某商品的利润在售价为 2 元、5 元、10 元时分别为 4 元、10 元、20 元，我们很容易得出商品的利润与售价的关系符合直线：y=2x. 在上面这个简单的一元线性回归方程中，我们称 “2” 为回归系数，即斜率为其回归系数，回归系数表示商品的售价(x)每变动一个单位，其利润(y)与之对应的变动关系。


    from numpy.linalg import inv  # 矩阵求逆
    from numpy import dot  # 矩阵点乘
    X = np.array([[1],[2],[3]])
    Y = 2*X
    #theta = (X.T*X)^-1X.T*Y
    theta = dot(dot(inv(dot(X.T,X)),X.T),Y)
    print(theta)

即X的转置与X点乘，再求逆， 再与X的转置点乘，再与Y点乘，一层层加括号即可。

python实现线性回归之最小二乘法，最小二乘法详解
https://blog.csdn.net/m0_38075425/article/details/90738415/

Thonny，更适合青少年学习编程的IDE工具。
https://zhuanlan.zhihu.com/p/464253266?utm_id=0

- Python 编程工具比较多样，像比较入门的 Mu Editor，项目在用的 Pycharm、Pydev 等等
- Thonny 由爱沙尼亚的 Tartu 大学开发，它的调试器是专为学习和教学编程而设计的，针对初学者学习 Python 语言优化了很多工具上的方式和方法

错误处理
Go

    func main() {
       f, err := ioutil.ReadFile("test.xml")
       if err != nil {
          fmt.Println(err)
       } else {
          fmt.Println(f)
       }
    }

Rust

    fn main() {
        let f = File::open("hello.txt");
        match f {
            Ok(file) => {
                println!("File opened successfully.");
            },
            Err(err) => {
                println!("Failed to open the file.");
            }
        }
    }

Java

    try {
        File file = new File("test.txt");
        FileReader reader = new FileReader(file);
    } catch (FileNotFoundException e) {
        System.out.println("文件不存在！");
    } catch (IOException e) {
        System.out.println("文件读取失败！");
    }

NodeJs

    fs.readFile('file.txt', 'utf8', function (err, data) {
        if (err) rerurn console.log(err);
        console.log(data)
    });

好的，对于普通可逆的矩阵，我们可以使用高斯-约当消元法来计算它的逆矩阵。这个算法包括三个步骤：

1. 构造增广矩阵：将一个大小为 (N, 2N) 的增广矩阵创建为原始矩阵和单位矩阵的水平拼接。
2. 对增广矩阵进行行变换：通过一系列行变换将增广矩阵变成上三角形矩阵。
3. 反向代入：通过反向代入的方式，将上三角形矩阵转换成单位矩阵，同时右侧的部分即为原始矩阵的逆矩阵。

下面是一个用 Rust 实现这个函数的例子：

```rust
fn matrix_inverse(matrix: Vec<Vec<f64>>) -> Option<Vec<Vec<f64>>> {
    let n = matrix.len();

    // 构造增广矩阵
    let mut augmented_matrix = vec![vec![0.0; 2 * n]; n];
    for i in 0..n {
        for j in 0..n {
            augmented_matrix[i][j] = matrix[i][j];
        }
        augmented_matrix[i][i + n] = 1.0;
    }

    // 行变换，将增广矩阵变成上三角形矩阵
    for i in 0..n {
        if augmented_matrix[i][i] == 0.0 {
            // 如果主对角线上的元素为零，则矩阵不可逆
            return None;
        }
        for j in i + 1..n {
            let ratio = augmented_matrix[j][i] / augmented_matrix[i][i];
            for k in 0..2 * n {
                augmented_matrix[j][k] -= ratio * augmented_matrix[i][k];
            }
        }
    }

    // 反向代入，将上三角形矩阵变成单位矩阵以及右侧的部分即为逆矩阵
    for i in (0..n).rev() {
        for j in 0..n {
            let mut sum = 0.0;
            for k in i + 1..n {
                sum += augmented_matrix[i][k] * augmented_matrix[k][j + n];
            }
            augmented_matrix[i][j + n] = (augmented_matrix[i][j + n] - sum) / augmented_matrix[i][i];
        }
        augmented_matrix[i][i] = 1.0; // 对角线上的元素变成1
    }

    // 提取逆矩阵部分并返回
    let mut inverse_matrix = vec![vec![0.0; n]; n];
    for i in 0..n {
        for j in 0..n {
            inverse_matrix[i][j] = augmented_matrix[i][j + n];
        }
    }

    Some(inverse_matrix)
}

fn main() {
    let matrix = vec![
        vec![1.0, 2.0],
        vec![3.0, 4.0],
    ];
    let inverse_matrix = matrix_inverse(matrix).unwrap();
    println!("{:?}", inverse_matrix);
}
```

你可以将你的矩阵作为参数传递给 `matrix_inverse` 函数，并得到其逆矩阵作为返回值。请注意，这个函数假设矩阵是可逆的，并且不对输入进行验证。

OAuth2.0 原理流程及其单点登录和权限控制
https://kefeng.wang/2018/04/06/oauth2-sso/

## 游戏

Rpg maker 教程1：RM系列软件异同
https://zhuanlan.zhihu.com/p/59197226


## java

https://blog.csdn.net/wdays83892469/article/details/126216765

Java AOT 在经过一波三折的发展后，目前最为成熟可行的方案就是 GraalVM Native Image，它所带来的优势是显著的：
- 更快的启动速度、
- 更小的内存消耗、
- 脱离JVM独立运行 。

但对应的，它也存在着非常多的限制，尤其是在充满了反射等动态特性的 Java 工程生态圈，很难得到大规模的广泛应用。

总的来说，Java AOT目前是有着明确使用场景的一种技术，主要可以应用于：

- 编写命令行 CLI 程序，希望程序可以完整独立运行而不是需要额外安装JVM。
- 运行环境资源严重受限的场景，例如 IoT 设备、边缘计算等场景。
- 希望追求极致的启动速度，并且应用逻辑相对足够轻量，如 FaaS。


Java 9 ~Java 17 主要更新了什么？
https://www.zhihu.com/question/513873484/answer/2328624634


Spring Boot 最新版3.x 集成 OAuth 2.0实现认证授权服务、第三方应用客户端以及资源服务
https://blog.csdn.net/friendlytkyj/article/details/128889875

https://blog.csdn.net/hxx688/article/details/127035172
https://blog.csdn.net/qq_50909707/article/details/127165284

Spring Security OAuth停更了？探索官方进化版Spring Authorization Server的革新之处！
https://blog.csdn.net/citywu123/article/details/131080947

OAuth2一些常见问题的QA
https://felord.cn/oauth2-qa.html

## 前端

Chrome 80.0中将SameSite的默认值设为Lax,对现有的Cookie使用有什么影响?
https://www.zhihu.com/question/373011996/answer/2442976854?utm_id=0

The Marketing Secrets Apple & Tesla Always Use: Rory Sutherland | E165
https://www.youtube.com/watch?v=Hz3RWxJck68

## 后台

Fast, secure data deduplication solutions
https://redis.com/solutions/use-cases/deduplication/

RedisBloom: Probabilistic Data Structures for Redis
https://github.com/RedisBloom/RedisBloom

bloom filter之所以能做到在时间和空间上的效率比较高，是因为牺牲了判断的准确率、删除的便利性

- 存在误判，可能要查到的元素并没有在容器中，但是 hash 之后得到的k个位置上值都是 1。
    - 如果 bloom filter 中存储的是黑名单，那么可以通过建立一个白名单来存储可能会误判的元素。
- 删除困难。一个放入容器的元素映射到 bit 数组的 k 个位置上是1，删除的时候不能简单的直接置为0，可能会影响其他元素的判断。
    - 可以采用Counting Bloom Filter

测试
235w url， 占内存才 14M

Redis：HyperLogLog使用与应用场景
https://zhuanlan.zhihu.com/p/445885116

布隆过滤器的误判率该如何计算？ - 领创集团Advance Group的回答 - 知乎
https://www.zhihu.com/question/38573286/answer/2750794079

过滤器中有 1 亿个元素, k 取值为 14，那么需要空间约为 20 亿 bit ( 238MB )
误报率是2.019*10^9

## 期权

买近卖远，卖近买远——好难把握的日历价差
http://options.hexun.com/2020-11-06/202380118.html

买近卖远
- 图形和买跨有些类似， 都是中间震荡会亏损，两边大幅突破会盈利。但是稍微带点曲度。
- 认购日历来说，如果大涨，希望近月涨的多，远月涨得少，那就赚了；
- 如果大跌，希望近月跌有限，远月跌的多点，那也还是赚的。
- 如果一直震荡，平值期权到期归零，远月还有价值，则整体可能亏损。        
- 买近卖远的目的还有一个
    - 就是希望在远月波动率比较高的时候（希腊字母vega相关），能赚取远月降波的收益
    - 同时，近月的gamma比较高，希望波动大点。

卖近买远

- 图形类似于卖跨的损益图。
- 希望标的近期震荡，然后再涨或者跌，把近月时间价值损耗完。
- 通常笔者用于卖方末日轮的对冲。
- 还可以赚取远月升波的收益。

策略难点
- 该组合的价差，在日内基本上横在一个数值附近，远月近月合约涨跌的金额基本对等~全天来看，收益极少。
- 日历策略的影响因素比较多：
    - 近月远月合约的涨跌金额
    - 近月远月合约的波动率升降规律
    - 上涨时谁上涨的多，下跌时谁下跌的少。
    - 标的窄幅震荡时，这个因素更没有可控性。

期权卖方终极指南-第七章：如何正确交易日历价差
https://zhuanlan.zhihu.com/p/443166257

本章将主要讨论做多日历价差，也就是买入远期的认购合约 卖出近月认购合约。

日历价差的最大亏损就是建仓付出的权利金之差，比如上述苹果的例子，每组损失 = 340美元。

平值日历价差的希腊字母如下：
- Delta 中性 - 建仓时我们对于股价运动方向不敏感
- 空 Gamma - 快速的上涨或下跌将使我们亏损
- 多 Theta - 其他条件不变时，随着时间的推移我们会挣钱
- 多 vega - 隐波上升时我们会挣钱

本质

- 认为日历价差实际上多 vega 实际上是大多数人犯的错误
- 日历价差的真正意义 - 针对远期隐波(Forward Volatility）下注
- 日历价差实际上是在交易 gamma 和 vega 的相对值。
- 可以认为日历价差做空 gamma 的同时用 vega 对冲，或者说针对远期隐波下注。
- 7 月合约的损益主要来自 gamma，而 9 月合约的损益主要来自 vega。
- 实际上，在日历价差中，波动率的变化带来的盈亏，会被期限结构完全对冲。

假如你得到消息某个制药公司将要在 9 月 1 日公布一个新药，此时不同合约的隐波如下：

七月 = 30%
八月 = 30%
九月 = 30%
十月 = 30%

- 现在我们知道 9 月份会有一个大新闻，其他人并不知道。因此 9 月合约的隐波应该比现在的更高才对，那么我们就卖出八月合约同时买入九月合约。
- 这样，当新药公布后 9 月合约的隐波将会上涨，我们的日历价差组合也会盈利。
- 究其原因，这种隐波的变化并非是跨期的，近月的隐波没有变化。我们建立组合时买入的远期隐波 = 30%，而八月结束后，隐波上升了，所以我们赚钱了。


波动率锥可以帮我们评估对于同一行权价的合约，哪个到期日的合约的隐波被高估或者低估。

- 比如说如果我们看到一张合约 30 日到期的隐波 = 80， 120 日到期的隐波 = 80，
- 由于期限结构的存在，我们可以认为 120 日的隐波被高估了，特别是当股票的实际波动 = 100时，30 日隐蔽显然被低估了。
- 对于这种情况，我们应该做空日历价差，卖出 120 日合约 + 买入 30 日合约。
- 这个组合表达的观点是，gamma 相对于 vega 更便宜，所以我们做多 gamma，做空 vega。


讲讲低风险套利——日历价差
https://zhuanlan.zhihu.com/p/375927451

什么是波动率？它代表市场上人心的看法，买的人多就会升波，卖的人多就会降波。

围绕做空波动率这个核心，我们开展开讲讲什么时候做正（卖近买远），什么时候做反（买近卖远）。

1. 如果股市一直打横，那所有合约波动率将趋于统一。股市震荡才是常态，高位似尖峰，终将回落。
2. 如果市场认为最近市场要跌（买沽多），沽合约近月合约波动率先行升波，远月合约波动率滞后后行，反之亦然
3. 如果市场认为最近市场跌无可跌（卖沽多），那沽合约近月波动率会降得比远月更低
4. 远月合约价值高，一旦升波增长幅度会更大，反之亦然
5. 近月合约时间价值消减数倍于远月

由上述原因我根据观察推出几个心得：

- 如果近月波高，远月波低，那我们做正日历（卖近买远）。正日历乃王道，它完美贴合第1、4、5点的理念。因此它收益是最高的。
- 如果近月波低，远月波高，那我们做反日历（买近卖远）。反日历做为正日历的补充，因为第2、3点的关系，它更温和稳健，回撤也更小。
- 市场不可能又看涨（购合约），又看跌（沽合约），所以它们购沽双方经常一个适合做正，一个适合做反。可互为补充来选择适合自己的组合。
