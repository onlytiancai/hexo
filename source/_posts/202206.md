---
title: '202206'
date: 2022-06-02 11:22:45
tags:
---

解读历史波动率，发现交易机会
https://new.qq.com/rain/a/20210830A08VW100

波动率具有聚集性、均值回归、长记忆性和非对称性等特征。

- 聚集性是指波动率具有高低波动率各自聚集的特征，即高波动率之后大概率还是高波动率，低波动率之后大概率还是低波动率，而且高波动率和低波动率聚集的时期会交替出现，呈现出的就是周期性。
- 均值回归是指波动率没有长期上涨或者下跌的趋势，而是围绕均值上下波动。
- 长记忆性指波动率存在较高的自相关特性，现在的波动率在很大程度上取决于其过去的波动率。
- 波动率的非对称性主要体现在隐含波动率上，即标的价格上涨或下跌相同幅度对期权隐含波动率的影响并不相同。

说到企业估值，你怎能不知道现金流折现法？
https://baijiahao.baidu.com/s?id=1722719168861071511

一个公司的内在价值，取决于该公司未来能产生的自由现金流。

自由现金流折现法（Discounted Cash Flow ），简称 DCF。

无杠杆自由现金流/企业自由现金流 
- = EBIT－调整的所得税+折旧摊销—营运资金的增加—资本支出。 
- = 净利润＋利息费用＋所得税－调整的所得税＋折旧摊销—营运资金的增加—资本支出。

---

为了防止 ssh 或 scp 时，提示是否需要确认 hosts，可以提前执行如下语句

    ssh-keyscan abc.com >> ~/.ssh/known_hosts

ssh 或 scp 自动输入密码可以使用 sshpass

    apt-get update
    apt-get  install -y --force-yes sshpass
    sshpass -p "xxxx" scp local remote 


---

Postgree 查看数据库，表，索引大小

查看各个库的大小

    select pg_database.datname, pg_size_pretty (pg_database_size(pg_database.datname)) AS size from pg_database;

查看 public 库下所有表索引大小

    select indexrelname, pg_size_pretty(pg_relation_size(relid)) from pg_stat_user_indexes where schemaname='public' order by pg_relation_size(relid) desc;

查看 public 库下所有表大小

    select relname, pg_size_pretty(pg_relation_size(relid)) from pg_stat_user_tables where schemaname='public' order by pg_relation_size(relid) desc;

查看单个表大小

    select pg_size_pretty(pg_relation_size('表名'));

查询所有表的大小并排序（包含索引）

    SELECT table_schema || '.' || table_name AS table_full_name, pg_size_pretty(pg_total_relation_size('"'
        || table_schema || '"."' || table_name || '"')) AS size
    FROM information_schema.tables
    ORDER BY
    pg_total_relation_size('"' || table_schema || '"."' || table_name || '"') DESC limit 20;


查询表大小按大小排序并分离data与index

    SELECT
    table_name,
    pg_size_pretty(table_size) AS table_size,
    pg_size_pretty(indexes_size) AS indexes_size,
    pg_size_pretty(total_size) AS total_size
    FROM (
    SELECT
    table_name,
    pg_table_size(table_name) AS table_size,
    pg_indexes_size(table_name) AS indexes_size,
    pg_total_relation_size(table_name) AS total_size
    FROM (
    SELECT ('"' || table_schema || '"."' || table_name || '"') AS table_name
    FROM information_schema.tables
    ) AS all_tables
    ORDER BY total_size DESC
    ) AS pretty_sizes;

---

分区，挂磁盘

    # 创建挂载目录
    mkdir /data_ext2/

    # 查看块设备，找到新硬盘
    lsblk
    file -s /dev/nvme2n1
        /dev/nvme2n1: data

    # 分区并查看
    mkfs -t xfs /dev/nvme2n1
    file -s /dev/nvme2n1
        /dev/nvme2n1: SGI XFS filesystem data (blksz 4096, inosz 512, v2 dirs)
    lsblk -f | grep nvme2n1
        nvme2n1     xfs                      de17daa1-9f10-4b60-adc9-2a7ed1557c89

    # 挂载并查看
    mount /dev/nvme2n1 /data_ext2
    df -h /data_ext2

    # 查看设备 id，并设置开机启动挂载
    blkid
    vi /etc/fstab
        UUID=7a0a651a-0000-0000-0000-717708af1dec /data_ext2  xfs  defaults 0 0


---


How to fix "MySQL server has gone away", "Packets out of order" and similar MySQL connection errors
https://www.ryadel.com/en/fix-mysql-server-gone-away-packets-order-similar-mysql-related-errors/

for these errors:
    
    MySQL server has gone away
    Error reading result set's header
    Error executing query
    MySQL server has gone away for query
    2006, MySQL server has gone away
    Packets out of order. Expected X received Y. Packet size=Z

- There's a good chance that the server is dropping an incorrect or too large packet sent by the client. To fix that, check the max_allowed_packet variable value and set it to a very high value
- The issue might be due to the fact that the server timed out and closed the connection. Check the wait_timeout MySQL variable value and ensure it's large enough. 
- Additionally, if you're getting the Packets out of order error in the PHP log, there's an high chance that the issue is due to the fact that there's an incompatibility between the PHP and MySQL versions you're using.

我的一个不常访问的页面使用的如下的代码

    $dbh = new PDO('mysql:host=localhost;dbname=test', $user, $pass, array(
        PDO::ATTR_PERSISTENT => true
    ));

但偶尔报下面的错误，不是经常

    warning: Packets out of order. Expected 1 received 0. Packet size=145

我怀疑是我使用了 MySQL 持久连接，但这个页面不经常使用，造成这个连接长时间保持但没有任何真实请求，然后被 MySQL 单方面杀掉了，但我过了很久之后再刷新页面，PHP 尝试复用这个连接时，被 MySQL 拒绝了。

如果是一个比较活跃的站点，持久连接带来的好处可能比较明显，因为节省了很多连接建立的花时间。但一个请求量比较低的页面没有必要使用持久连接，不知道理解的对不对。

要想真正避免这个问题，我想在执行 SQL 之前要先执行个空的 MySQL ping 命令，验证 MySQL 连接是否有效，如果无效则重新连接，有效则直接使用。这个验证连接的操作可以设置一个缓存时间，比如 1 小时内不需要再次验证，以避免频繁的做无用功。

---

## sql

TODO: MySQL窗口函数OVER()
https://blog.csdn.net/weixin_46544385/article/details/120609601

mysql 快速查看表的数据行数

    set session information_schema_stats_expiry=0;
    show table status like 'mytable'\G


Postgres array_to_string() and array_agg() to decouple your interface
https://pemungkah.com/postgres-array_to_string-and-array_agg-to-decouple-your-interface/

    SELECT Array_to_string(Array_agg(number_of
                                     || ' '
                                     || status), ',  ') summary_status,
           Max(user_id) USER
    FROM   (SELECT Count(*) number_of,
                   status,
                   user_id
            FROM   jobs
            GROUP  BY status,
                      user_id
            ORDER  BY user_id) AS foo
    GROUP  BY user_id;

PostgreSQL:基于数组(外键)列联接2个表
http://ask.sov5.cn/q/rxkaSsHjMA

  SELECT u.id, u.name,array_agg(g.name) group_names FROM users u JOIN groups g ON g.id = ANY (u.group_ids)
  GROUP BY u.id, u.name;

postgree 里的换行

    select 'test line 1'||E'\n'||'test line 2';

postgree 的表 a 有一个数组列 id_arr，格式类似于 `[1,2,3]`，关联到 b 表的 id 列，我想得到 `[a,b,c]`

    SELECT (select string_agg(b.name,',') from b where b.id = ANY(a.ids)) FROM a 


创建临时表，当你断开与数据库的连接后，临时表就会自动被销毁。 临时表只在当前连接中有效。

    CREATE TEMPORARY TABLE 临时表名 AS
    (
        SELECT *  FROM 旧的表名
        LIMIT 0,10000
    );

MySQL 全文索引
https://blog.csdn.net/dreamyuzhou/article/details/120432893

- 全文索引只能用于InnoDB或MyISAM表，并且只能为CHAR、VARCHAR或TEXT列创建。
- MySQL提供了一个内置的全文ngram解析器，支持中文，日文和韩文(CJK)，以及一个可安装的MeCab日文全文解析器插件。 “ngram全文解析器”和“MeCab全文解析器插件”
- FULLTEXT索引定义可以在创建表时在CREATE TABLE语句中给出，也可以稍后使用ALTER TABLE或CREATE index添加。
- 对于大型数据集，将数据加载到一个没有FULLTEXT索引的表中，然后在此之后创建索引，比将数据加载到一个已有FULLTEXT索引的表中要快得多。
- 分区表不支持全文搜索

mysql的全文索引只有一种方法判断相关性，就是词频，索引并不会记录匹配的词在字符串中的位置。并且，全文索引和数据量有较大的关系，全文索引只会全部在内存中时，性能才会很好，因此当全文索引过大，不能全部读入进内存，性能就会比较差。
可以通过一下点，思考下全文索引的问题：

- 修改一段文本中的100个单词时，需要索引100次。
- 全文索引的长度对性能的影响也是巨大的
- 全文索引会产生更多的碎片，需要频繁的优化（optimize table）操作
- 内存和数据容量也是常可观，所以需要规划和参数控制这部分
- 因为mysql复制机制是基于逻辑复制，产生的binlog很大，那就会出现主从延迟等问题。
- 词分割（token_size）也是一个问题，单个汉字也能表达出不同的意思。
- 查询上：如果sql中包含match against，而索引列上又正好有全文索引，那么mysql就一定会使用全文索引，如果此时还有其他索引，mysql也不会去对比那个索引性能更高。

MySQL 创建全文索引

    -- 创建全文索引
    CREATE FULLTEXT INDEX idx ON table_name(`columns`);

    -- 创建表时自动加全文索引，临时表不支持
    CREATE TABLE articles (
      id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY,
      title VARCHAR(200),
      body TEXT,
      FULLTEXT (title,body)
    ) ENGINE=InnoDB;

    -- 插入测试数据
    INSERT INTO articles (title,body) VALUES
    ('MySQL Tutorial','DBMS stands for DataBase ...'),
    ('How To Use MySQL Well','After you went through a ...'),
    ('Optimizing MySQL','In this tutorial, we show ...'),
    ('1001 MySQL Tricks','1. Never run mysqld as root. 2. ...'),
    ('MySQL vs. YourSQL','In the following database comparison ...'),
    ('MySQL Security','When configured properly, MySQL ...');
    
    -- 简单全文搜索 
    SELECT * FROM articles
    WHERE MATCH (title,body)
    AGAINST ('database' IN NATURAL LANGUAGE MODE);

    +----+-------------------+------------------------------------------+
    | id | title             | body                                     |
    +----+-------------------+------------------------------------------+
    |  1 | MySQL Tutorial    | DBMS stands for DataBase ...             |
    |  5 | MySQL vs. YourSQL | In the following database comparison ... |
    +----+-------------------+------------------------------------------+


    -- 布尔搜索
    SELECT * FROM articles WHERE MATCH (title,body)
    AGAINST ('+MySQL -YourSQL' IN BOOLEAN MODE);

    +----+-----------------------+-------------------------------------+
    | id | title                 | body                                |
    +----+-----------------------+-------------------------------------+
    |  6 | MySQL Security        | When configured properly, MySQL ... |
    |  1 | MySQL Tutorial        | DBMS stands for DataBase ...        |
    |  2 | How To Use MySQL Well | After you went through a ...        |
    |  3 | Optimizing MySQL      | In this tutorial, we show ...       |
    |  4 | 1001 MySQL Tricks     | 1. Never run mysqld as root. 2. ... |
    +----+-----------------------+-------------------------------------+

    -- 查询扩展搜索是对自然语言搜索的修改。
    -- 搜索字符串用于执行自然语言搜索，然后将搜索返回的最相关行的单词添加到搜索字符串中，并再次执行搜索。
    -- 查询返回第二次搜索的行.
    SELECT * FROM articles
    WHERE MATCH (title,body)
        AGAINST ('database' WITH QUERY EXPANSION);

    +----+-----------------------+------------------------------------------+
    | id | title                 | body                                     |
    +----+-----------------------+------------------------------------------+
    |  5 | MySQL vs. YourSQL     | In the following database comparison ... |
    |  1 | MySQL Tutorial        | DBMS stands for DataBase ...             |
    |  3 | Optimizing MySQL      | In this tutorial, we show ...            |
    |  6 | MySQL Security        | When configured properly, MySQL ...      |
    |  2 | How To Use MySQL Well | After you went through a ...             |
    |  4 | 1001 MySQL Tricks     | 1. Never run mysqld as root. 2. ...      |
    +----+-----------------------+------------------------------------------+

---

## laravel

https://zhuanlan.zhihu.com/p/95558910
Laravel 一主多从配置

    'mysql' => [
        'write' => [
            'host' => '192.168.1.180',
        ],
        'read' => [
            ['host' => '192.168.1.182'],
            ['host' => '192.168.1.179'],
        ],
        'driver' => 'mysql',
        'database' => 'database',
        'username' => 'root',
        'password' => '',
        'charset' => 'utf8',
        'collation' => 'utf8_unicode_ci',
        'prefix' => '',
    ]


https://lqwang.net/13.html

- 使用 chunkById 或者 chunk 方法的时候不要添加自定义的排序，chunk和chunkById的区别就是chunk是单纯的通过偏移量来获取数据，chunkById进行了优化，不使用偏移量，使用 id 过滤，性能提升巨大。在数据量大的时候，性能可以差到几十倍的样子。
- 而且使用chunk在更新的时候，也会遇到数据会被跳过的问题。
- 同时 chunkById 在你没有传递 column 参数时，会默认添加 order by id，可能会遇到索引失效的问题。解决办法就是传递 column 参数即可。
- 本人感觉 chunkById 不光是根据 Id 分块，而是可以根据某一字段进行分块，这个字段是可以指定的。


https://laravel.com/docs/master/broadcasting
https://learnku.com/docs/laravel/9.x/broadcasting/12223

For example, imagine your application is able to export a user's data to a CSV file and email it to them. However, creating this CSV file takes several minutes so you choose to create and mail the CSV within a queued job. When the CSV has been created and mailed to the user, we can use event broadcasting to dispatch a App\Events\UserDataExported event that is received by our application's JavaScript. Once the event is received, we can display a message to the user that their CSV has been emailed to them without them ever needing to refresh the page.

- Laravel 通过 WebSocket 连接使它很容易的去「广播」您的服务端 Laravel 事件。
- 广播 Laravel 事件允许您在服务器端 Laravel 应用程序和客户端 JavaScript 应用程序之间共享相同的事件名称和数据。
- 广播背后的核心概念很简单：客户端连接到前端的命名频道，而您的 Laravel 应用程序将事件广播到后端的这些频道。
- laravel-websockets 和 soketi 包为 Laravel 提供了与 Pusher 兼容的 WebSocket 服务器。 这些包允许您在没有商业 WebSocket 提供程序的情况下利用 Laravel 广播的全部功能。


---

## 算法

群蚁算法、遗传算法、模拟退火算法，禁忌搜索算法等通俗详解
https://blog.51cto.com/u_13682052/2981273

单只蚂蚁的行为及其简单，行为数量在10种以内，但成千上万只蚂蚁组成的蚁群却能拥有巨大的智慧，这离不开它们信息传递的方式——信息素。蚂蚁在行走过程中会释放一种称为“信息素”的物质，用来标识自己的行走路径。在寻找食物的过程中，根据信息素的浓度选择行走的方向，并最终到达食物所在的地方。信息素会随着时间的推移而逐渐挥发。在一开始的时候，由于地面上没有信息素，因此蚂蚁们的行走路径是随机的。蚂蚁们在行走的过程中会不断释放信息素，标识自己的行走路径。随着时间的推移，有若干只蚂蚁找到了食物，此时便存在若干条从洞穴到食物的路径。由于蚂蚁的行为轨迹是随机分布的，因此在单位时间内，短路径上的蚂蚁数量比长路径上的蚂蚁数量要多，从而蚂蚁留下的信息素浓度也就越高。这为后面的蚂蚁们提供了强有力的方向指引，越来越多的蚂蚁聚集到最短的路径上去。


继续考虑寻找f(x)最大值的问题，爬山算法搜索到A点时就会停止搜索，原因是A点左右的值均小于A点的值。模拟退火算法采用的解决办法是以一定的概率选择A两边的点，尽管A两边的点并不是局部最优解，这样就有一定的概率搜索到D点，从而搜索到B点，最终获得了全局最优解。上文中的一定概率来自于固体退火原理：当固体温度较高时，物质内能较大，固体内部分子运动剧烈；当温度逐渐降低时，物体内能也随之降低，分子运动趋于平稳；当固体温度降到常温时，固体内部分子运动最终平稳。根据Metropolis准则，粒子在温度T时趋于平衡的概率为e^(-ΔE/(kT))，其中E为温度T时的内能，ΔE为其改变量，k为Boltzmann常数。


TODO: numpy实现朴素贝叶斯模型（高斯分布）
https://www.jianshu.com/p/efa980944235

    import numpy as np


    class GaussianNB():

        def fit(self, X, y):
            """模型拟合"""
            self.y_prior = [round(sum(y == i) / len(y), 6) for i in sorted(set(y))]
            self.features_param = []

            for i in sorted(set(y)):
                pos = np.where(y == i)
                features_data = X[pos]
                features_mean = np.mean(features_data, axis=0)
                features_std = np.std(features_data, axis=0)

                param = [(round(avg, 6), round(std, 6)) for avg, std in zip(features_mean, features_std)]
                self.features_param.append(param)

        def predict(self, x):
            """模型预测"""
            result = []
            for i in range(x.shape[0]):
                bayes_prob = []

                for j in range(len(self.y_prior)):
                    x_param = self.features_param[j]
                    y_param = self.y_prior[j]
                    xi_conditional_prob = 1

                    for k in range(len(x_param)):
                        xi_conditional_prob *= self.gauss_pro(x[i][k], x_param[k][0], x_param[k][1])
                    bayes_prob.append(round(y_param * xi_conditional_prob, 6))
                result.append(np.where(bayes_prob == np.max(bayes_prob))[0][0])

            return np.array(result)

        def gauss_pro(self, v, miu, sigma):
            """高斯分布概率密度计算"""
            part1 = 1 / (sigma * np.sqrt(2 * np.pi))
            part2 = np.exp(-1 * (v - miu) ** 2 / (2 * sigma ** 2))
            return round(part1 * part2, 6)


    if __name__ == '__main__':
        from sklearn import datasets

        iris = datasets.load_iris()
        X = iris.data
        y = iris.target

        gnb = GaussianNB()
        gnb.fit(X, y)

        res = gnb.predict(X)
        print(res)

## Neo4j

neo4j使用文档
https://www.cnblogs.com/naimao/p/13497046.html

- 它是一个嵌入式的、基于磁盘的、具备完全的事务特性的Java持久化引擎，
- 它将结构化数据存储在网络(从数学角度叫做图)上而不是表中。
- 图形数据库数据模型的主要构建块是：节点、关系、属性
- neo4j主要存储节点和关系，其中关系必须为有向关系，描述节点和关系的数据以属性的形式存储，节点和关系上都能放键值对的属性。不同类型的节点和关系通过标签Label来区别，不同标签的节点代表不同类型节点，不同标签关系代表不同类型关系

创建一个标签为Person的节点，其有属性 name 和age：

    create (:Person{name:'小红',age:21});

查询一个节点：

    match (m:Person{name:'小红',age:21}) return n;

删除一个节点：

    match (m:Person{name:'小红',age:21}) delete n;



创建关系：

    create (a:Person{name:"a"}),(b:Person{name:"b"}) with a,b create (a)-[r:Friend]->(b);

查询关系：

    match (a:Person{name:"a"})-[r:Friend]->(b:Person{name:"b"}) return r;

删除关系：

    match p=(a:Person{name:"a"})-[r:Friend]->(b:Person{name:"b"}) delete p;

---

## linux

Linux--网络通信命令(给其它用户发送广播消息)
https://blog.csdn.net/qq_42119367/article/details/123427804

    wall Happy New Year

http://ipcmen.com/nano

nano 是一个字符终端的文本编辑器，有点像 DOS 下的 editor 程序。它比 vi/vim 要简单得多，比较适合Linux初学者使用。

- 移动光标：使用用方向键移动。
- 复制一整行：Alt+6
- 剪贴一整行：Ctrl+K
- 粘贴：Ctrl+U
- 如果需要复制／剪贴多行或者一行中的一部分
    - 先将光标移动到需要复制／剪贴的文本的开头
    - 按 Ctrl+6（或者 Alt+A ）做标记
    - 然后移动光标到 待复制／剪贴的文本末尾。这时选定的文本会反白
    - 用 Alt+6 来复制，Ctrl+K 来剪贴。
    - 若在选择文本过程中要取消，只需要再按一次 Ctrl+6。
- 退出: Ctrl+X: 如
    - 果你修改了文件，下面会询问你是否需要保存修改。
    - 输入Y确认保存，输入N不保存，
    - 按 Ctrl+C 取消返回。
    - 如果输入了Y，下一步会让你输入想要保存的文件名。
        - 如果不需要修改文件名直接回车就行；
        - 若想要保存成别的名字（也就是另存为）则输入新名称然后确定。
        - 这个时候也可用 Ctrl+C 来取消返回。
- 撤销：Alt+U
- 重做: Alt+E

Linux visudo配置详解
http://t.zoukankan.com/wutao1935-p-10045809.html

sudo的工作过程如下：

- 当用户执行 sudo 时，系统会主动寻找 /etc/sudoers 文件，判断该用户是否有执行 sudo 的权限
- 确认用户具有可执行 sudo 的权限后，让用户输入用户自己的密码确认
- 若密码输入成功，则开始执行 sudo 后续的命令
- root 执行 sudo 时不需要输入密码 (sudoers 文件中有配置 root ALL=(ALL) ALL这样一条规则)
- 若欲切换的身份与执行者的身份相同，也不需要输入密码

visudo 使用 vi 打开 /etc/sudoers文件，但是在保存退出时，visudo 会检查内部语法，避免用户输入错误信息

    # 允许 user1 用户执行任意路径下的任意命令
    user1 ALL=(ALL) ALL
    # 允许 user1 用户不输入该用户的密码的情况下使用所有命令
    user1 ALL=(ALL) NOPASSWD: ALL
    # 允许 user1 用户执行特定命令
    user1 ALL=/sbin/mount /mnt/cdrom, /sbin/umount /mnt/cdrom

---

## 量化

从零入门量化交易系列(十二)Black-Litterman模型及python实现
https://zhuanlan.zhihu.com/p/363540266

这里还要再说一个重要的假设就是共轭分布，千万别被共轭（conjugate）这个词吓到，它几乎是整个Black-Litterman模型在形式上的核心。原因在于一个正态分布乘以另一个正态分布结果还依然是正态分布。这就是为什么模型不仅要假设\mu服从正态分布，连投资者的观点也要假设为正态分布的原因。这两个正态分布都在分子上，相乘后结果仍为正态分布，而分母上的积分为常数，并不影响新的正态分布的均值和方差。也就是说先验和后验同属于正态分布，两者共轭，因此后验分布的均值方差都可以根据公式快速得到。这也就方便我们得到后验分布的均值，也就是我们想要的期望收益率。

【量化模型】Black-Litterman模型介绍
https://zhuanlan.zhihu.com/p/25041459



Markowitz的MPT模型：给定风险水平下的预期收益最大化，也可以是其对偶命题，给定预期收益 水平下的风险最小化。

- 缺点：
    - 假设不成立：MPT理论假设为投资者一致且理性（即投资者对预期收益、标准差和风险资产相关性具有一致预测；投资者行为遵循最优化原则，即投资者理性）。现实中显然不成立。
    - 无仓位限制：在无卖空限制条件下，MPT模型经常导致在一些资产上有很大的空头头寸，而实际上大量投资者具有卖空约束。中国市场对卖空进行限制，模型经常导致在某些资产上权重为零，而在另一些资产上权重过大，即出现资产配置过于集中的现象。
    - 参数敏感：对输入参数如预期收益率作小幅度变化，可能导致模型结果发生剧烈变化。
- 改进：
    - 高盛的Fisher Black和Robert Litterman在研究中发现，对组合中德国债券预期报酬率做0.1%小幅修正后，竟然该类资产的投资比例由原来的10.0%提高至55.0%。
    - 做法：加入投资者自己的观点，而且有一定的置信水平。二人提出了BL模型：使用Bayes定理（条件概率），构建收益时通过一定方式对市场隐含收益率与主观预期收益的加权平均。



Black-Litterman模型是基于MPT基础上的资产配置理论。BL模型在隐含市场收益率和分析师主观预测信息的基础上，成功解决了MPT模型中假设条件不成立，参数敏感等问题。

- BL优化后确实战胜市场均衡配置；
- BL模型同样也存在模型上的缺陷，需要继续改进。历史数据计算出来的协方差矩阵在长期内可能不能良好刻画出短期关系，需要动态调整；分析师主观预期（看法）信心水平的设定具有很大的主观 随意性，在方法上还存在众多分歧；
- BL模型比较符合目前国内基金投资真实市场环境，如关注分析师主观预期，存在投资仓位上下限规定等；
- BL模型适用于行业资产配置，而一般不配置具体的投资，倾向区分大类资产；但有的地方也说可以用在个股组合中；
- 达里奥桥水基金的全球配置更关注风险因子，而非大类。股票中的能源股和部分大宗商品息息相关，从而分配到相同的一类中。是不同于MPT和BL的资产配置理论

大奖章基金
https://baijiahao.baidu.com/s?id=1664399262080636944&wfr=spider&for=pc

- 该基金的收费比较高，管理费为 5%，业绩分成比例为 36%。
- 奖章基金的管理规模在 100 亿美金左右，折合人民币约 700 亿。
- 二十年年化收益率近 70%

【矩阵分析】Condition Number
https://zhuanlan.zhihu.com/p/81053589

- 我们在衡量某个函数敏感度与稳定性时，常使用的一种方法是求导。即观察导数[公式] 的大小。
- 然而对于一个矩阵而言，所谓的导数似乎就不是那么显著，在此我们就介绍一个常用的指标矩阵敏感度指标：Condition Number。

TODO: 因子分析的数学基础
https://zhuanlan.zhihu.com/p/348566335

- 1.1 方差
- 1.2 标准差
- 1.3 均方误差
- 1.4 python实现
- 1.5 协方差
- 1.6 相关系数
- 1.7 特征值和特征向量
- 1.8 使用Python求解特征值和特征向量
- 1.9逆矩阵和转置矩阵
- 1.10 矩阵的迹和行列式

多元函数泰勒级数展开_用Python学微积分(4)---泰勒级数
https://blog.csdn.net/weixin_39906358/article/details/111639431

---

WaveFunctionCollapse
https://github.com/mxgmn/WaveFunctionCollapse

动态生成迷宫

---

## shell

查看 json 压缩文件里倒数第 2 行的 id， 因为有可能是个末尾损坏的压缩包，所以跳过倒数第一条数据

    zcat ./out_000000168373001.gz 2>/dev/null | tail -n2 | head -n1| jq -r '.id'

---

https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html

Think before disabling the _source field
Users often disable the _source field without thinking about the consequences, and then live to regret it. If the _source field isn’t available then a number of features are not supported:

- The update, update_by_query, and reindex APIs.
- On the fly highlighting.
- The ability to reindex from one Elasticsearch index to another, either to change mappings or analysis, or to upgrade an index to a new major version.
- The ability to debug queries or aggregations by viewing the original document used at index time.
- Potentially in the future, the ability to repair index corruption automatically.


https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-id-field.html

- Each document has an _id that uniquely identifies it, which is indexed so that documents can be looked up either with the GET API or the ids query. 
- The _id can either be assigned at indexing time, or a unique _id can be generated by Elasticsearch. 
- This field is not configurable in the mappings.

---

https://www.517712.com/gupiao/94304.html

美元加息对黄金影响的三种情况：

1、美元加息利好黄金：美元加息，但货币并未成功回流流回银行系统，而企业贷款利率上涨，贷款困难，就会出现流动性危机，美元会被看空，对黄金就是利好。
2、美元加息利空黄金：美元加息，存款利率上涨，货币顺利回流银行系统，美元升值，对黄金就是利空。
3、美国加息，美元升值，但同时出现地区性动荡、石油危机等情况影响正常经济发展，对黄金有利好作用，就是美元和黄金同时上涨。


国际现货黄金是以美元来定价的，因此美元和黄金价格呈现一定的负相关关系，换句话说当美元上涨时，黄金价格大概率是下跌，而当美元下跌时，黄金价格大概率是上涨。

---

NGINX Proxy to wordpress website
https://stackoverflow.com/questions/38205743/nginx-proxy-to-wordpress-website

    location ^~ /blog/ {
      proxy_pass http://127.0.0.1:8080/;
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $http_host;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto $scheme;
    }

    /** set the site URL */
    define('WP_HOME','http://www.example.com/blog');
    define('WP_SITEURL','http://www.example.com/blog');

    /** Fix to get the dashboard working with the reverse proxy.*/
    $_SERVER['REQUEST_URI'] = str_replace("/wp-admin/", "/blog/wp-admin/",  $_SERVER['REQUEST_URI']);


Gutenberg breaks completely if site URL is not the same as wordpress URL
https://github.com/WordPress/gutenberg/issues/1761    


---


反射放大攻击
https://baijiahao.baidu.com/s?id=1730701810134759899

所谓的反射放大攻击是非常常见的DDoS攻击手法，其基本原理非常简单：攻击者通过控制僵尸网络伪造靶机IP向特定的公网服务器发送请求，公网服务器收到请求后会向靶机发送更大的应答报文，从而实现攻击流量放大。

这里的公网服务器是指对外开放某些可被利用作反射放大的协议端口的服务器，比较常见的协议有DNS、NTP、SNMP、Memcached等，这些协议一般基于UDP，并且协议本身存在缺陷，没有校验来源IP的真实性，且存在应答报文远大于请求报文等特点。这种反射放大手法简单、有效，一直深受黑客喜爱，所以很长一段时间内UDP反射就是反射放大攻击的别称。

早在2018年就出现利用公网服务器开放的TCP端口进行反射攻击的手法，相比UDP反射放大攻击，此类利用TCP协议栈的反射攻击实际并无太明显的流量放大效果，因为请求的来源IP是伪造的，无法与TCP服务器完成TCP三次握手建立连接，所以无法得到应用层的应答报文。但是这种攻击利用了TCP的协议栈特性，使靶机看到攻击流量具备协议栈行为，而且成份复杂(synack、ack、rst等混合流量)，导致反向挑战、协议栈行为校验等传统的TCP防护算法无法防护，大大增加了防护难度，所以这种TCP反射诞生后很快成为DDoS攻击的主流攻击手法。

放大系数可以理解为流量的放大倍数，计算方法非常简单，就是response总长度/query总长度。传统的UDP反射攻击的放大系数与具体的协议实现相关，所以放大系数是一个相对固定的值：除了Memcached反射攻击以外，其他UDP反射放大系数不超过600，而且以200以内为主。

---

## nginx

A Guide to Caching with NGINX and NGINX Plus
https://www.nginx.com/blog/nginx-caching-guide/

## docker

docker数据卷与数据卷容器以及备份与恢复
https://blog.csdn.net/m0_60360828/article/details/122641289

创建容器

    docker run -it -v /opt --name test_1 centos:7 /bin/bash

        echo "123321" > /opt/1.txt

备份

    docker run -it --volumes-from test_1 -v /mnt:/mnt centos:7 tar cvf /mnt/opt.tar /opt

- `--volumes-from test_1` ：指定数据卷容器所在
- `-v /mnt:/mnt`：共享该容器中mnt目录到主机的mnt
- `tar cvf /mnt/opt.tar /opt`：这个较为绕，mnt是该容器跟主机之间共享的一个目录。所以将备份后的数据放到这个文件中，好备份之后直接导到主机。
- 后面跟随的备份目标路径，则是该容器与数据卷容器之间的共享目录，因为要对数据卷容器进行备份，所以则是需要备份该目录

查看备份

    ls /mnt/
    tar xvf opt.tar
    cat ./opt/1.txt

恢复

    #创建数据卷容器，并且共享opt目录
    docker run -it -v /opt --name test_2 nginx:1.12 /bin/bash

    #创建一个容器，作用是从主机中获取文件，并且将该文件传输给数据卷容器
    docker run -itd --volumes-from test_2 -v /mnt:/mnt nginx:1.12 tar xvf /mnt/opt.tar -C /opt/

    cd opt/ && ls
    cat ./opt/1.txt

## elasticsearch 

Field data typesedit
https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-types.html

Each field has a field data type, or field type. This type indicates the kind of data the field contains, such as strings or boolean values, and its intended use. For example, you can index strings to both text and keyword fields. However, text field values are analyzed for full-text search while keyword strings are left as-is for filtering and sorting.


